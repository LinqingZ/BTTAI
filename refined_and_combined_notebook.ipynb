{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jObCfGtDTsip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4664a955-ec3d-4718-e72d-accf54002ba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [Connecting to security.ubuntu.com (185.125.190.39)] [Connected to cloud.r-p\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.39)] [\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to security.ubun\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "\r0% [1 InRelease gpgv 242 kB] [Connecting to security.ubuntu.com (185.125.190.39\r                                                                               \rHit:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Waiting for headers] [Conne\r                                                                               \rHit:6 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to ppa.launchpad\r                                                                               \rIgn:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 242 kB] [Waiting for headers] [Connecting to ppa.launchpad\r                                                                               \rHit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.00~git2288-10f4998a-2).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.7/dist-packages (0.3.10)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.7/dist-packages (from pytesseract) (9.3.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.7/dist-packages (from pytesseract) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=21.3->pytesseract) (3.0.9)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!sudo apt install tesseract-ocr\n",
        "!pip install pytesseract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils\n",
        "!pip install pdf2image"
      ],
      "metadata": {
        "id": "vufwZI7tT9Z5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17ed7d53-efdb-4b94-db61-35b985bf8685"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 15 not upgraded.\n",
            "Need to get 154 kB of archives.\n",
            "After this operation, 613 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 poppler-utils amd64 0.62.0-2ubuntu2.14 [154 kB]\n",
            "Fetched 154 kB in 0s (1,896 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 124038 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_0.62.0-2ubuntu2.14_amd64.deb ...\n",
            "Unpacking poppler-utils (0.62.0-2ubuntu2.14) ...\n",
            "Setting up poppler-utils (0.62.0-2ubuntu2.14) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pdf2image\n",
            "  Downloading pdf2image-1.16.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from pdf2image) (9.3.0)\n",
            "Installing collected packages: pdf2image\n",
            "Successfully installed pdf2image-1.16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytesseract\n",
        "import PIL.Image\n",
        "from pytesseract import image_to_data, Output\n",
        "from pdf2image import convert_from_path, convert_from_bytes\n",
        "from IPython.display import display, Image\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "id": "xHADnTLKUARq"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "e6h6q-w2UBOv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "743b737e-1ca3-430f-fa87-818c5973edf5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# file_path = \"/content/drive/MyDrive/RUBICONMD /Data/\" # Hoai's drive path\n",
        "file_path = \"/content/drive/MyDrive/000ABTT/RUBICONMD /Data/\" # Linqing's drive path"
      ],
      "metadata": {
        "id": "AJHwVi4RHhv6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Extracting text and performing image preprocessing based on an accuracy score threshold**"
      ],
      "metadata": {
        "id": "lXIaTmknjssP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compare Pytessaract with EasyOCR to see which one does better and and the result suggests that Pytessaract performs a bit better and faster at 0.93 compared to 0.85 from Easy OCR.**\n",
        "\n",
        "**We will set a THRESHOLD for the confidence score to decide if it's good enough to move on to AWS model. If not, perform image preprocessing and compare it to the confidence score before. If better but still below 0.90, try EasyOCR and compare 3 scores together.**"
      ],
      "metadata": {
        "id": "7hEYoXmlAFpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a helper function with regex to clean text\n",
        "def clean(x):\n",
        "  clean_x = re.sub(r'[^a-zA-Z0-9]+', '', x)\n",
        "  return clean_x\n",
        "\n",
        "\"\"\"\n",
        "Define a preprocessing function to extract text based on the type of input file (images or pdfs)\n",
        "\"\"\"\n",
        "\n",
        "def preprocessing(path, threshold):\n",
        "  extension = path[-3:]\n",
        "\n",
        "  if extension == 'pdf':\n",
        "    img = convert_from_path(path, fmt='PNG')[0]\n",
        "    img = np.array(img)   # change type to np array from PIL image so that it works for smoothening after\n",
        "  else:\n",
        "    img = cv.imread(path)\n",
        "    \n",
        "  ocr_result_df = pytesseract.image_to_data(img, output_type='data.frame')\n",
        "\n",
        "  ocr_mean = ocr_result_df.conf.mean()\n",
        "\n",
        "  print(\"Shape of this data frame: \" + str(ocr_result_df.shape))\n",
        "  ocr_result_df = ocr_result_df[(ocr_result_df.conf != -1) & (ocr_result_df.text != ' ')]\n",
        "  print(\"Shape of the data frame after removing cells with no text detected (-1): \" + str(ocr_result_df.shape))\n",
        "  print(\"The mininum score: \" + str(ocr_result_df.conf.min()))\n",
        "  print(\"The maximum core: \" + str(ocr_result_df.conf.max()))\n",
        "  print(\"The mean score: \" + str(ocr_mean))\n",
        "  \n",
        "  df_to_be_returned = ocr_result_df\n",
        "\n",
        "  # check if the text extraction accuracy meets the threshold\n",
        "  if ocr_mean < OCR_ACC_THRESHOLD:\n",
        "    print(\"The mean accuracy score is below the threshold of \" + str(OCR_ACC_THRESHOLD) +\", so now try image preprocessing.\")\n",
        "    # denoising of image saving it into dst image\n",
        "    dst = cv.fastNlMeansDenoisingColored(img, None, 10, 10, 7, 15)\n",
        "    #text_dst = pytesseract.image_to_string(dst)\n",
        "    #text_dst = text_dst.replace('\\n', '')\n",
        "    text_dst_df = pytesseract.image_to_data(dst, output_type='data.frame')\n",
        "    text_dst_df = text_dst_df[(text_dst_df.conf != -1) & (text_dst_df.text != ' ')]\n",
        "    print(\"The new mininum score: \" + str(text_dst_df.conf.min()))\n",
        "    print(\"The new maximum core: \" + str(text_dst_df.conf.max()))\n",
        "    print(\"The new average confidence score after smoothening the image: \" + str(text_dst_df.conf.mean()))\n",
        "\n",
        "    if ocr_mean < text_dst_df.conf.mean(): # check again if the smoothening method actually helps\n",
        "      df_to_be_returned = text_dst_df\n",
        "  else:\n",
        "    print(\"Image preprocessing is not useful here. Use the default extraction instead.\")\n",
        "\n",
        "  df_to_be_returned['text_clean'] = df_to_be_returned['text'].apply(clean)\n",
        "  \n",
        "  return list(df_to_be_returned.text), df_to_be_returned\n",
        "  # a complete data frame is also returned here for Linqing to use later for her work, use text_clean column Linqing!\n"
      ],
      "metadata": {
        "id": "uqbHZApSUUAf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from os import nice\n",
        "# Approach: use image_to_data instead to obtain statistics of each word, then take the mean score of words \n",
        "# to obtain line's confidence score\n",
        "\n",
        "OCR_ACC_THRESHOLD = 95.0\n",
        "\n",
        "sample_test_result = f'{file_path}Copy of Sample_EKG.jpeg'\n",
        "nicer_pdf = f'{file_path}AAA Physician AI Research Opportunity!!!.pdf'\n",
        "copy_lab_report_pdf = f'{file_path}Copy of Sample_LabReport.pdf'\n",
        "sample_xray_image = f'{file_path}Copy of Sample_XRay.jpg'\n",
        "ARXIV_V5_CHESTXRAY_pdf = f'{file_path}ARXIV_V5_CHESTXRAY.pdf'\n",
        "README_CHESTXRAY_pdf = f'{file_path}README_CHESTXRAY.pdf'\n",
        "\n",
        "# LINQING: please use the data_df here for your part\n",
        "data, data_df = preprocessing(nicer_pdf, OCR_ACC_THRESHOLD)"
      ],
      "metadata": {
        "id": "RwpVbzB4YwZp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d45d98ec-c6b1-4f7e-c5d4-e9a420a456dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of this data frame: (551, 12)\n",
            "Shape of the data frame after removing cells with no text detected (-1): (463, 12)\n",
            "The mininum score: 0\n",
            "The maximum core: 97\n",
            "The mean score: 78.55535390199637\n",
            "The mean accuracy score is below the threshold of 95.0, so now try image preprocessing.\n",
            "The new mininum score: 0\n",
            "The new maximum core: 97\n",
            "The new average confidence score after smoothening the image: 93.62554112554112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "WWmAvambhRSg",
        "outputId": "3c40bb01-4d79-4a9f-a335-67e28d464317"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    level  page_num  block_num  par_num  line_num  word_num  left  top  width  \\\n",
              "4       5         1          1        1         1         1  1362  208     69   \n",
              "6       5         1          1        1         2         1  1323  216     33   \n",
              "7       5         1          1        1         2         2  1437  220     25   \n",
              "9       5         1          1        1         3         1  1307  238     19   \n",
              "13      5         1          2        1         1         1   515  271    193   \n",
              "14      5         1          2        1         1         2   722  271     70   \n",
              "15      5         1          2        1         1         3   811  271    151   \n",
              "16      5         1          2        1         1         4   979  271    177   \n",
              "17      5         1          2        1         1         5  1292  264     20   \n",
              "19      5         1          2        1         2         1   416  322    156   \n",
              "\n",
              "    height  conf       text text_clean  \n",
              "4       16    75  SERVICES,   SERVICES  \n",
              "6       28    18         os         os  \n",
              "7       25     4         84         84  \n",
              "9       23    68          =             \n",
              "13      41    95  Radiology  Radiology  \n",
              "14      32    96        and        and  \n",
              "15      41    96    Imaging    Imaging  \n",
              "16      32    96   Sciences   Sciences  \n",
              "17      53     0          F          F  \n",
              "19      32    96   National   National  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-580b279c-15b4-4ee8-839d-570d0190c559\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>level</th>\n",
              "      <th>page_num</th>\n",
              "      <th>block_num</th>\n",
              "      <th>par_num</th>\n",
              "      <th>line_num</th>\n",
              "      <th>word_num</th>\n",
              "      <th>left</th>\n",
              "      <th>top</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>conf</th>\n",
              "      <th>text</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1362</td>\n",
              "      <td>208</td>\n",
              "      <td>69</td>\n",
              "      <td>16</td>\n",
              "      <td>75</td>\n",
              "      <td>SERVICES,</td>\n",
              "      <td>SERVICES</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1323</td>\n",
              "      <td>216</td>\n",
              "      <td>33</td>\n",
              "      <td>28</td>\n",
              "      <td>18</td>\n",
              "      <td>os</td>\n",
              "      <td>os</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1437</td>\n",
              "      <td>220</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>4</td>\n",
              "      <td>84</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1307</td>\n",
              "      <td>238</td>\n",
              "      <td>19</td>\n",
              "      <td>23</td>\n",
              "      <td>68</td>\n",
              "      <td>=</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>515</td>\n",
              "      <td>271</td>\n",
              "      <td>193</td>\n",
              "      <td>41</td>\n",
              "      <td>95</td>\n",
              "      <td>Radiology</td>\n",
              "      <td>Radiology</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>722</td>\n",
              "      <td>271</td>\n",
              "      <td>70</td>\n",
              "      <td>32</td>\n",
              "      <td>96</td>\n",
              "      <td>and</td>\n",
              "      <td>and</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>811</td>\n",
              "      <td>271</td>\n",
              "      <td>151</td>\n",
              "      <td>41</td>\n",
              "      <td>96</td>\n",
              "      <td>Imaging</td>\n",
              "      <td>Imaging</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>979</td>\n",
              "      <td>271</td>\n",
              "      <td>177</td>\n",
              "      <td>32</td>\n",
              "      <td>96</td>\n",
              "      <td>Sciences</td>\n",
              "      <td>Sciences</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>1292</td>\n",
              "      <td>264</td>\n",
              "      <td>20</td>\n",
              "      <td>53</td>\n",
              "      <td>0</td>\n",
              "      <td>F</td>\n",
              "      <td>F</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>416</td>\n",
              "      <td>322</td>\n",
              "      <td>156</td>\n",
              "      <td>32</td>\n",
              "      <td>96</td>\n",
              "      <td>National</td>\n",
              "      <td>National</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-580b279c-15b4-4ee8-839d-570d0190c559')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-580b279c-15b4-4ee8-839d-570d0190c559 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-580b279c-15b4-4ee8-839d-570d0190c559');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Text processing with NLTK**"
      ],
      "metadata": {
        "id": "iGYio_Q0jnDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "J5V9cYj8mkTp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fca3365-aa1c-498e-8900-94747878da6c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "MgKGtPQbZ9Gd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeecadb3-8d2f-4323-a973-2ab16aa331e1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing stop words, punctuations, POS Tagging and Lemmatization**\n",
        "\n"
      ],
      "metadata": {
        "id": "XnJlSASPmpI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_processing(data):\n",
        "  # stop_words = set(stopwords.words('english'))\n",
        "  stop_words = stopwords.words('english')\n",
        "\n",
        "  # add new stopwords to the list\n",
        "  stop_words.extend([\"could\",\"though\",\"would\",\"also\",\"many\",'much'])\n",
        "\n",
        "  #word_tokens = word_tokenize(data)\n",
        "  word_tokens = []\n",
        "  for token in data:\n",
        "    # remove any value that are not alphanumerical\n",
        "    new_token = re.sub(r'[^a-zA-Z0-9]+', '', token)\n",
        "    if new_token != \"\":\n",
        "      word_tokens.append(new_token)\n",
        "\n",
        "  # Remove the stopwords from the list of tokens\n",
        "  filtered_tokens = [w for w in word_tokens if not w.lower() in stop_words]\n",
        "\n",
        "  print(\"List of words before filtering: \")\n",
        "  print(word_tokens)\n",
        "  print(\"List of words after filtering: \")\n",
        "  print(filtered_tokens)\n",
        "  print('Word count: ' + str(len(word_tokens)))\n",
        "  print('Word count after filtering: ' + str(len(filtered_tokens)))\n",
        "\n",
        "  data_tagset = nltk.pos_tag(filtered_tokens)\n",
        "  df_tagset = pd.DataFrame(data_tagset, columns=['Word', 'Tag'])  \n",
        "\n",
        "  # Create lemmatizer object \n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  # Lemmatize each word and display the output\n",
        "  lemmatize_text = []\n",
        "  for word in filtered_tokens:\n",
        "    output = [word, lemmatizer.lemmatize(word, pos='n'), lemmatizer.lemmatize(word, pos='a'), lemmatizer.lemmatize(word, pos='v')]\n",
        "    lemmatize_text.append(output)\n",
        "  # create DataFrame using original words and their lemma words\n",
        "  df = pd.DataFrame(lemmatize_text, columns =['Word', 'Lemmatized Noun', 'Lemmatized Adjective', 'Lemmatized Verb']) \n",
        "\n",
        "  df['Tag'] = df_tagset['Tag']\n",
        "\n",
        "  # replace with single character for simplifying\n",
        "  df = df.replace(['NN','NNS','NNP','NNPS'],'n')\n",
        "  df = df.replace(['JJ','JJR','JJS'],'a')\n",
        "  df = df.replace(['VBG','VBP','VB','VBD','VBN','VBZ'],'v')\n",
        "  \n",
        "  # define a function where take the lemmatized word when tagset is noun, and take lemmatized adjectives when tagset is adjective\n",
        "  \n",
        "  df_lemmatized = df.copy()\n",
        "  df_lemmatized['Tempt Lemmatized Word']=df_lemmatized['Lemmatized Noun'] + ' | ' + df_lemmatized['Lemmatized Adjective']+ ' | ' + df_lemmatized['Lemmatized Verb']\n",
        "  df_lemmatized.head(5)\n",
        "  lemma_word = df_lemmatized['Tempt Lemmatized Word']\n",
        "  tag = df_lemmatized['Tag']\n",
        "  i = 0\n",
        "  new_word = []\n",
        "  while i<len(tag):\n",
        "      words = lemma_word[i].split('|')\n",
        "      if tag[i] == 'n':        \n",
        "          word = words[0]\n",
        "      elif tag[i] == 'a':\n",
        "          word = words[1]\n",
        "      elif tag[i] == 'v':\n",
        "          word = words[2]\n",
        "      new_word.append(word)\n",
        "      i += 1\n",
        "  df_lemmatized['Lemmatized Word'] = new_word\n",
        "  data_string = \" \".join(df_lemmatized['Lemmatized Word'].unique())\n",
        "\n",
        "  print(\"Word count after lemmatization: \" + str(len(df_lemmatized['Lemmatized Word'].unique())))\n",
        "  return data_string, df_lemmatized\n"
      ],
      "metadata": {
        "id": "yybPrzQggJOg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "OCR_ACC_THRESHOLD = 95.0\n",
        "\n",
        "sample_test_result = f'{file_path}Copy of Sample_EKG.jpeg'\n",
        "nicer_pdf = f'{file_path}AAA Physician AI Research Opportunity!!!.pdf'\n",
        "copy_lab_report_pdf = f'{file_path}Copy of Sample_LabReport.pdf'\n",
        "sample_xray_image = f'{file_path}Copy of Sample_XRay.jpg'\n",
        "ARXIV_V5_CHESTXRAY_pdf = f'{file_path}ARXIV_V5_CHESTXRAY.pdf'\n",
        "README_CHESTXRAY_pdf = f'{file_path}README_CHESTXRAY.pdf'\n",
        "\n",
        "img_path = copy_lab_report_pdf\n",
        "\n",
        "data, data_df = preprocessing(img_path, OCR_ACC_THRESHOLD)\n",
        "\n",
        "print(\"-----------------------------------------------------\")\n",
        "\n",
        "data_string, lemmatized_df = text_processing(data)\n",
        "# returned data needs to be one string to be used in AWS\n",
        "# data_string is input for AWS in next step"
      ],
      "metadata": {
        "id": "xSdQDF2NyqN0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af5cf4f6-d06c-4c27-ca3d-9dd228bf5218"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of this data frame: (448, 12)\n",
            "Shape of the data frame after removing cells with no text detected (-1): (377, 12)\n",
            "The mininum score: 4\n",
            "The maximum core: 97\n",
            "The mean score: 77.64508928571429\n",
            "The mean accuracy score is below the threshold of 95.0, so now try image preprocessing.\n",
            "The new mininum score: 0\n",
            "The new maximum core: 97\n",
            "The new average confidence score after smoothening the image: 90.21148825065274\n",
            "-----------------------------------------------------\n",
            "List of words before filtering: \n",
            "['From', 'unknown', 'Page', '44', 'Date', '12282016', '355', '17', 'PM', 'Page', '004', 'Alpha', 'Beta', 'Medical', 'Center', 'Laboratory', 'Ashland', 'Campus', 'SURGICAL', 'PATHOLOGY', 'REPORT', 'Patient', 'SANDBERG', 'KATIE', 'Specimen', 'ABS1600001', 'The', 'immunoperoxidase', 'stamns', 'reported', 'above', 'was', 'developed', 'and', 'its', 'performance', 'charactensties', 'determined', 'by', 'Pathologics', 'Services', 'Inc', 'Berkeley', 'CA', 'It', 'has', 'not', 'been', 'cleared', 'or', 'approved', 'by', 'the', 'US', 'Food', 'and', 'Drug', 'Admumstration', 'although', 'such', 'approval', 'is', 'not', 'required', 'for', 'analytespecific', 'reagents', 'of', 'this', 'type', 'MYD88', 'MUTATION', 'ANALYSIS', 'Both', 'technical', 'and', 'professional', 'components', 'performed', 'at', 'Genomia', 'X', 'International', 'Laboratories', 'MOL', '1685', '183', 'and', 'reported', 'Not', 'Detected', 'INTERPRETATION', 'MYD88', 'mutation', 'is', 'the', 'most', 'frequent', 'genomic', 'abnormality', 'in', 'diffuse', 'large', 'celllymphoma', 'DLBCL', 'activated', 'Bcelllike', 'ABC', 'subtype', 'detected', 'in', '40o', 'of', 'cases', 'MYD88', 'is', 'rarely', 'mutated', 'in', 'the', 'germinal', 'center', 'Bcelllike', 'GCB', 'DLBL', 'therefore', 'it', 'can', 'be', 'used', 'to', 'differentiate', 'between', 'the', 'two', 'subtypes', 'MYD88', 'mutation', 'is', 'detected', 'in', 'approximately', '90o', 'of', 'cases', 'of', 'Waldenstrom', 'macroglobulinemialymphoplasmacytic', 'lymphoma', 'MYD88', 'mutation', 'analysis', 'can', 'be', 'a', 'useful', 'prognostic', 'tool', 'for', 'patients', 'with', 'IgMMGUS', 'since', 'the', 'L265P', 'mutation', 'is', 'associated', 'with', 'a', 'higher', 'risk', 'of', 'disease', 'progression', 'and', 'a', 'greater', 'disease', 'burden', 'MYD88', 'mutation', 'has', 'also', 'been', 'reported', 'to', 'be', 'common', '40', 'in', 'central', 'nervous', 'system', 'lymphoma', 'INTRAOPERATIVE', 'CONSULTATION', 'LEFT', 'OCCIPITAL', 'LOBE', 'FROZEN', 'AND', 'TOUCH', 'PREP', 'MILD', 'GLIAL', 'PROLIFERATION', 'CANNOT', 'EXCLUDE', 'LOWGRADE', 'GLIOMA', 'REFERRED', 'TO', 'PERMANENT', 'LL', 'GROSS', 'DESCRIPTION', 'rt', 'A', 'Received', 'fresh', 'labeled', 'left', 'occipital', 'lobe', 'are', 'three', 'tanred', 'dull', 'soft', 'fragments', 'of', 'tissue', '04', 'x', '02', 'x', '02', 'cm', '06', 'x', '05', 'x', '02', 'cm', 'and', '05', 'x', '02', 'x', '01', 'cm', 'A', 'touch', 'prep', 'and', 'frozen', 'section', 'is', 'performed', 'on', 'the', 'entire', 'specimen', 'the', 'frozen', 'section', 'remnant', 'is', 'transferred', 'in', 'its', 'entirety', 'to', 'cassettes', 'Al', 'No', 'additional', 'tissue', 'is', 'identified', 'within', 'the', 'container', 'B', 'Received', 'fresh', 'labeled', 'left', 'occipital', 'lobe', 'are', 'three', 'tanred', 'soft', 'irregular', 'fragments', 'of', 'tissue', '05', 'x', '02', 'x', '02', 'cm', '06', 'x', '03', 'x', '02', 'cm', 'and', '04', 'x', '03', 'x', '02', 'cm', 'The', 'specimen', 'is', 'filtered', 'wrapped', 'and', 'submitted', 'in', 'toto', 'in', 'B1', 'KB', '1292016', '0139', 'PM', 'Final', 'Diagnosis', 'performed', 'by', 'Ellen', 'Barns', 'MD', 'Electronically', 'signed', '12282016', 'Slides', 'reviewed', 'at', 'XYZ', 'Bravi', 'Medical', 'Center', '2450', 'Ashby', 'Avenue', 'Berkeley', 'CA', '94705', '5106440951', 'CLIA', 'ID', '1200011555', 'Page', '3', 'of', '3', 'This', 'fax', 'was', 'received', 'by', 'GFl', 'FAXmaker', 'fax', 'server', 'For', 'more', 'information', 'visit', 'http', 'Awww', 'gficom']\n",
            "List of words after filtering: \n",
            "['unknown', 'Page', '44', 'Date', '12282016', '355', '17', 'PM', 'Page', '004', 'Alpha', 'Beta', 'Medical', 'Center', 'Laboratory', 'Ashland', 'Campus', 'SURGICAL', 'PATHOLOGY', 'REPORT', 'Patient', 'SANDBERG', 'KATIE', 'Specimen', 'ABS1600001', 'immunoperoxidase', 'stamns', 'reported', 'developed', 'performance', 'charactensties', 'determined', 'Pathologics', 'Services', 'Inc', 'Berkeley', 'CA', 'cleared', 'approved', 'US', 'Food', 'Drug', 'Admumstration', 'although', 'approval', 'required', 'analytespecific', 'reagents', 'type', 'MYD88', 'MUTATION', 'ANALYSIS', 'technical', 'professional', 'components', 'performed', 'Genomia', 'X', 'International', 'Laboratories', 'MOL', '1685', '183', 'reported', 'Detected', 'INTERPRETATION', 'MYD88', 'mutation', 'frequent', 'genomic', 'abnormality', 'diffuse', 'large', 'celllymphoma', 'DLBCL', 'activated', 'Bcelllike', 'ABC', 'subtype', 'detected', '40o', 'cases', 'MYD88', 'rarely', 'mutated', 'germinal', 'center', 'Bcelllike', 'GCB', 'DLBL', 'therefore', 'used', 'differentiate', 'two', 'subtypes', 'MYD88', 'mutation', 'detected', 'approximately', '90o', 'cases', 'Waldenstrom', 'macroglobulinemialymphoplasmacytic', 'lymphoma', 'MYD88', 'mutation', 'analysis', 'useful', 'prognostic', 'tool', 'patients', 'IgMMGUS', 'since', 'L265P', 'mutation', 'associated', 'higher', 'risk', 'disease', 'progression', 'greater', 'disease', 'burden', 'MYD88', 'mutation', 'reported', 'common', '40', 'central', 'nervous', 'system', 'lymphoma', 'INTRAOPERATIVE', 'CONSULTATION', 'LEFT', 'OCCIPITAL', 'LOBE', 'FROZEN', 'TOUCH', 'PREP', 'MILD', 'GLIAL', 'PROLIFERATION', 'CANNOT', 'EXCLUDE', 'LOWGRADE', 'GLIOMA', 'REFERRED', 'PERMANENT', 'GROSS', 'DESCRIPTION', 'rt', 'Received', 'fresh', 'labeled', 'left', 'occipital', 'lobe', 'three', 'tanred', 'dull', 'soft', 'fragments', 'tissue', '04', 'x', '02', 'x', '02', 'cm', '06', 'x', '05', 'x', '02', 'cm', '05', 'x', '02', 'x', '01', 'cm', 'touch', 'prep', 'frozen', 'section', 'performed', 'entire', 'specimen', 'frozen', 'section', 'remnant', 'transferred', 'entirety', 'cassettes', 'Al', 'additional', 'tissue', 'identified', 'within', 'container', 'B', 'Received', 'fresh', 'labeled', 'left', 'occipital', 'lobe', 'three', 'tanred', 'soft', 'irregular', 'fragments', 'tissue', '05', 'x', '02', 'x', '02', 'cm', '06', 'x', '03', 'x', '02', 'cm', '04', 'x', '03', 'x', '02', 'cm', 'specimen', 'filtered', 'wrapped', 'submitted', 'toto', 'B1', 'KB', '1292016', '0139', 'PM', 'Final', 'Diagnosis', 'performed', 'Ellen', 'Barns', 'MD', 'Electronically', 'signed', '12282016', 'Slides', 'reviewed', 'XYZ', 'Bravi', 'Medical', 'Center', '2450', 'Ashby', 'Avenue', 'Berkeley', 'CA', '94705', '5106440951', 'CLIA', 'ID', '1200011555', 'Page', '3', '3', 'fax', 'received', 'GFl', 'FAXmaker', 'fax', 'server', 'information', 'visit', 'http', 'Awww', 'gficom']\n",
            "Word count: 380\n",
            "Word count after filtering: 281\n",
            "Word count after lemmatization: 184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_df # this df is used to retrieve the location of the words (potential PHI according to AWS model) on the image, for redaction."
      ],
      "metadata": {
        "id": "2Z0q9NZBL2rb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "6b095bab-2d33-4968-dd79-ca205b53c5b2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     level  page_num  block_num  par_num  line_num  word_num  left   top  \\\n",
              "4        5         1          1        1         1         1   493     4   \n",
              "5        5         1          1        1         1         2   565     5   \n",
              "6        5         1          1        1         1         3   720     4   \n",
              "7        5         1          1        1         1         4   790     5   \n",
              "8        5         1          1        1         1         5   880     4   \n",
              "..     ...       ...        ...      ...       ...       ...   ...   ...   \n",
              "449      5         1         14        1         1        12   984  2278   \n",
              "450      5         1         14        1         1        13  1120  2279   \n",
              "451      5         1         14        1         1        14  1177  2278   \n",
              "452      5         1         14        1         1        15  1224  2278   \n",
              "453      5         1         14        1         1        16  1300  2278   \n",
              "\n",
              "     width  height  conf          text   text_clean  \n",
              "4       55      20    73         From.         From  \n",
              "5       97      19    74       unknown      unknown  \n",
              "6       56      25    95          Page         Page  \n",
              "7       34      18    95           4/4           44  \n",
              "8       51      20    94          Date         Date  \n",
              "..     ...     ...   ...           ...          ...  \n",
              "449    126      21    96  information,  information  \n",
              "450     42      18    84         visit        visit  \n",
              "451     40      23    30          http         http  \n",
              "452     70      19    30        //Awww         Awww  \n",
              "453     81      24    65       gfi.com       gficom  \n",
              "\n",
              "[383 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1397dfbb-6f0f-4797-9ee5-cc1495c2f1e2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>level</th>\n",
              "      <th>page_num</th>\n",
              "      <th>block_num</th>\n",
              "      <th>par_num</th>\n",
              "      <th>line_num</th>\n",
              "      <th>word_num</th>\n",
              "      <th>left</th>\n",
              "      <th>top</th>\n",
              "      <th>width</th>\n",
              "      <th>height</th>\n",
              "      <th>conf</th>\n",
              "      <th>text</th>\n",
              "      <th>text_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>493</td>\n",
              "      <td>4</td>\n",
              "      <td>55</td>\n",
              "      <td>20</td>\n",
              "      <td>73</td>\n",
              "      <td>From.</td>\n",
              "      <td>From</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>565</td>\n",
              "      <td>5</td>\n",
              "      <td>97</td>\n",
              "      <td>19</td>\n",
              "      <td>74</td>\n",
              "      <td>unknown</td>\n",
              "      <td>unknown</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>720</td>\n",
              "      <td>4</td>\n",
              "      <td>56</td>\n",
              "      <td>25</td>\n",
              "      <td>95</td>\n",
              "      <td>Page</td>\n",
              "      <td>Page</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>790</td>\n",
              "      <td>5</td>\n",
              "      <td>34</td>\n",
              "      <td>18</td>\n",
              "      <td>95</td>\n",
              "      <td>4/4</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>880</td>\n",
              "      <td>4</td>\n",
              "      <td>51</td>\n",
              "      <td>20</td>\n",
              "      <td>94</td>\n",
              "      <td>Date</td>\n",
              "      <td>Date</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>984</td>\n",
              "      <td>2278</td>\n",
              "      <td>126</td>\n",
              "      <td>21</td>\n",
              "      <td>96</td>\n",
              "      <td>information,</td>\n",
              "      <td>information</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>1120</td>\n",
              "      <td>2279</td>\n",
              "      <td>42</td>\n",
              "      <td>18</td>\n",
              "      <td>84</td>\n",
              "      <td>visit</td>\n",
              "      <td>visit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1177</td>\n",
              "      <td>2278</td>\n",
              "      <td>40</td>\n",
              "      <td>23</td>\n",
              "      <td>30</td>\n",
              "      <td>http</td>\n",
              "      <td>http</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>1224</td>\n",
              "      <td>2278</td>\n",
              "      <td>70</td>\n",
              "      <td>19</td>\n",
              "      <td>30</td>\n",
              "      <td>//Awww</td>\n",
              "      <td>Awww</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1300</td>\n",
              "      <td>2278</td>\n",
              "      <td>81</td>\n",
              "      <td>24</td>\n",
              "      <td>65</td>\n",
              "      <td>gfi.com</td>\n",
              "      <td>gficom</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>383 rows × 13 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1397dfbb-6f0f-4797-9ee5-cc1495c2f1e2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1397dfbb-6f0f-4797-9ee5-cc1495c2f1e2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1397dfbb-6f0f-4797-9ee5-cc1495c2f1e2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **AWS Comprehend Medical**"
      ],
      "metadata": {
        "id": "-jUHDIZ9B72Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install presidio-image-redactor\n",
        "!pip install botocore boto3"
      ],
      "metadata": {
        "id": "727qVR5UB7A-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "161d1ef1-c631-49f1-c726-02022ec98341"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting presidio-image-redactor\n",
            "  Downloading presidio_image_redactor-0.0.44-py3-none-any.whl (10.0 kB)\n",
            "Collecting presidio-analyzer>=1.9.0\n",
            "  Downloading presidio_analyzer-2.2.30-py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from presidio-image-redactor) (9.3.0)\n",
            "Collecting pytesseract==0.3.7\n",
            "  Downloading pytesseract-0.3.7.tar.gz (13 kB)\n",
            "Collecting pydantic==1.7.4\n",
            "  Downloading pydantic-1.7.4-cp37-cp37m-manylinux2014_x86_64.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 81.1 MB/s \n",
            "\u001b[?25hCollecting matplotlib==3.3.4\n",
            "  Downloading matplotlib-3.3.4-cp37-cp37m-manylinux1_x86_64.whl (11.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.5 MB 60.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->presidio-image-redactor) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->presidio-image-redactor) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->presidio-image-redactor) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->presidio-image-redactor) (3.0.9)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.3.4->presidio-image-redactor) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.3.4->presidio-image-redactor) (4.1.1)\n",
            "Collecting tldextract\n",
            "  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from presidio-analyzer>=1.9.0->presidio-image-redactor) (6.0)\n",
            "Collecting phonenumbers>=8.12\n",
            "  Downloading phonenumbers-8.13.1-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 57.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from presidio-analyzer>=1.9.0->presidio-image-redactor) (3.4.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from presidio-analyzer>=1.9.0->presidio-image-redactor) (2022.6.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.3.4->presidio-image-redactor) (1.15.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (8.1.5)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (4.64.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (2.0.7)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (2.4.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (3.0.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (2.11.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (1.0.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (21.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (3.0.8)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (3.3.0)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (1.0.3)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (2.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (0.10.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (0.8.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (3.10.0)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (5.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (1.24.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (0.0.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.7/dist-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (0.7.9)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.7/dist-packages (from typer<0.8.0,>=0.3.0->spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy>=3.2.0->presidio-analyzer>=1.9.0->presidio-image-redactor) (2.0.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract->presidio-analyzer>=1.9.0->presidio-image-redactor) (3.8.0)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
            "Building wheels for collected packages: pytesseract\n",
            "  Building wheel for pytesseract (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytesseract: filename=pytesseract-0.3.7-py2.py3-none-any.whl size=13954 sha256=54593da8dd48c74255289ee3bd779b9be0d9f9c5a715e3eb2eb840fd3c700ec7\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/71/6c/7a8c5ca2e699752506999ae7baeb692e2b4fc6488c2cddcb22\n",
            "Successfully built pytesseract\n",
            "Installing collected packages: pydantic, requests-file, tldextract, phonenumbers, pytesseract, presidio-analyzer, matplotlib, presidio-image-redactor\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 1.10.2\n",
            "    Uninstalling pydantic-1.10.2:\n",
            "      Successfully uninstalled pydantic-1.10.2\n",
            "  Attempting uninstall: pytesseract\n",
            "    Found existing installation: pytesseract 0.3.10\n",
            "    Uninstalling pytesseract-0.3.10:\n",
            "      Successfully uninstalled pytesseract-0.3.10\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "Successfully installed matplotlib-3.3.4 phonenumbers-8.13.1 presidio-analyzer-2.2.30 presidio-image-redactor-0.0.44 pydantic-1.7.4 pytesseract-0.3.7 requests-file-1.5.1 tldextract-3.4.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "pytesseract"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting botocore\n",
            "  Downloading botocore-1.29.19-py3-none-any.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 13.9 MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "  Downloading boto3-1.26.19-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 81.7 MB/s \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 77.9 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.15.0)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.13 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.26.19 botocore-1.29.19 jmespath-1.0.1 s3transfer-0.6.0 urllib3-1.26.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ACCESS_KEY = 'AKIAQD3URR2OBH2QXDO6'\n",
        "SECRET_KEY = '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'"
      ],
      "metadata": {
        "id": "dctdepEOB6-R"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from botocore.vendored import requests\n",
        "import json\n",
        "import boto3\n",
        "import logging\n",
        "import threading\n",
        "client = boto3.client(region_name='us-east-1', service_name='comprehendmedical', aws_access_key_id=ACCESS_KEY,\n",
        "    aws_secret_access_key=SECRET_KEY)\n",
        "\n",
        "def extract_entities_from_message(message):\n",
        "    return client.detect_phi(Text=message)\n",
        "\n",
        "#calling API, returned object is a dictionary of words that's classified as PHI and their confidence scores.\n",
        "result = extract_entities_from_message(data_string)"
      ],
      "metadata": {
        "id": "kchRpaPPB67n"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "iV_44pTOB65T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e52a1337-d49a-4c58-9d7d-46c95a6da0f5"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Entities': [{'Id': 1,\n",
              "   'BeginOffset': 56,\n",
              "   'EndOffset': 104,\n",
              "   'Score': 0.24228699505329132,\n",
              "   'Text': 'Laboratory  Ashland  Campus  SURGICAL  PATHOLOGY',\n",
              "   'Category': 'PROTECTED_HEALTH_INFORMATION',\n",
              "   'Type': 'ADDRESS',\n",
              "   'Traits': []},\n",
              "  {'Id': 2,\n",
              "   'BeginOffset': 123,\n",
              "   'EndOffset': 138,\n",
              "   'Score': 0.6523721218109131,\n",
              "   'Text': 'SANDBERG  KATIE',\n",
              "   'Category': 'PROTECTED_HEALTH_INFORMATION',\n",
              "   'Type': 'NAME',\n",
              "   'Traits': []},\n",
              "  {'Id': 3,\n",
              "   'BeginOffset': 150,\n",
              "   'EndOffset': 160,\n",
              "   'Score': 0.6089950799942017,\n",
              "   'Text': 'ABS1600001',\n",
              "   'Category': 'PROTECTED_HEALTH_INFORMATION',\n",
              "   'Type': 'ID',\n",
              "   'Traits': []},\n",
              "  {'Id': 4,\n",
              "   'BeginOffset': 248,\n",
              "   'EndOffset': 269,\n",
              "   'Score': 0.31890633702278137,\n",
              "   'Text': 'Pathologics  Services',\n",
              "   'Category': 'PROTECTED_HEALTH_INFORMATION',\n",
              "   'Type': 'ADDRESS',\n",
              "   'Traits': []},\n",
              "  {'Id': 5,\n",
              "   'BeginOffset': 271,\n",
              "   'EndOffset': 284,\n",
              "   'Score': 0.453013151884079,\n",
              "   'Text': 'Inc  Berkeley',\n",
              "   'Category': 'PROTECTED_HEALTH_INFORMATION',\n",
              "   'Type': 'ADDRESS',\n",
              "   'Traits': []},\n",
              "  {'Id': 6,\n",
              "   'BeginOffset': 1464,\n",
              "   'EndOffset': 1476,\n",
              "   'Score': 0.9972018003463745,\n",
              "   'Text': 'Ellen  Barns',\n",
              "   'Category': 'PROTECTED_HEALTH_INFORMATION',\n",
              "   'Type': 'NAME',\n",
              "   'Traits': []},\n",
              "  {'Id': 7,\n",
              "   'BeginOffset': 1525,\n",
              "   'EndOffset': 1551,\n",
              "   'Score': 0.44941285252571106,\n",
              "   'Text': 'Bravi  Ashby  Avenue  CLIA',\n",
              "   'Category': 'PROTECTED_HEALTH_INFORMATION',\n",
              "   'Type': 'ADDRESS',\n",
              "   'Traits': []},\n",
              "  {'Id': 8,\n",
              "   'BeginOffset': 1620,\n",
              "   'EndOffset': 1638,\n",
              "   'Score': 0.5228620171546936,\n",
              "   'Text': 'http  Awww  gficom',\n",
              "   'Category': 'PROTECTED_HEALTH_INFORMATION',\n",
              "   'Type': 'URL',\n",
              "   'Traits': []}],\n",
              " 'ModelVersion': '1.1.0',\n",
              " 'ResponseMetadata': {'RequestId': '59aee201-393c-4c63-8a97-b5295fb2187a',\n",
              "  'HTTPStatusCode': 200,\n",
              "  'HTTPHeaders': {'x-amzn-requestid': '59aee201-393c-4c63-8a97-b5295fb2187a',\n",
              "   'content-type': 'application/x-amz-json-1.1',\n",
              "   'content-length': '1400',\n",
              "   'date': 'Wed, 30 Nov 2022 06:02:38 GMT'},\n",
              "  'RetryAttempts': 0}}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result['Entities'][0]"
      ],
      "metadata": {
        "id": "c-CC--rAsnTs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c892a7e6-034a-4ac3-f146-3ef59f3d9602"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Id': 1,\n",
              " 'BeginOffset': 56,\n",
              " 'EndOffset': 104,\n",
              " 'Score': 0.24228699505329132,\n",
              " 'Text': 'Laboratory  Ashland  Campus  SURGICAL  PATHOLOGY',\n",
              " 'Category': 'PROTECTED_HEALTH_INFORMATION',\n",
              " 'Type': 'ADDRESS',\n",
              " 'Traits': []}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# set a baseline for the score to determine if it should be considered a PHI\n",
        "PHI_THRESHOLD = 0.5   # needs revision\n",
        "list_redact = []      # a list that holds all words to be redacted\n",
        "\n",
        "# convert the result dictionary into a data frame\n",
        "PHI_text_score = pd.DataFrame.from_dict(result['Entities'])\n",
        "\n",
        "# if the length of the 'Entities' in the returned dictionary is 0 or the max score doesn't exceed the threshold, \n",
        "# is it safe to conclude that there is no PHI/PII in the file?\n",
        "if len(PHI_text_score) == 0 or PHI_text_score.Score.max() < PHI_THRESHOLD:\n",
        "  print(\"There is no PHI/PII detected in the file.\")\n",
        "  # end of process\n",
        "\n",
        "else:  # perform redaction for the ones with score higher than the baseline\n",
        "  for i in PHI_text_score.index:\n",
        "    if PHI_text_score['Score'][i] > PHI_THRESHOLD:\n",
        "      #print(\"This entity needs to be redacted: \" + str(PHI_text_score['Text'][i]))\n",
        "      list_redact.append(PHI_text_score['Text'][i].split()) # I noticed that AWS will return a group of words that seem to link together,\n",
        "      # so here I split them for the redaction process after as it will redact one word at a time\n",
        "  list_redact = [item for sublist in list_redact for item in sublist] # flatten the list\n",
        "list_redact"
      ],
      "metadata": {
        "id": "Ey0n-cYZB62s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2698766-39da-4773-c3c2-e158ca497864"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SANDBERG', 'KATIE', 'ABS1600001', 'Ellen', 'Barns', 'http', 'Awww', 'gficom']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Locating and bordering words for the redaction process**\n",
        "**Draw Rectangle box on the image**"
      ],
      "metadata": {
        "id": "eSJDrXZJ6tBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### create a sample data of the tesseract output dataframe example"
      ],
      "metadata": {
        "id": "-llZo46bpKlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "from PIL import ImageDraw\n",
        "import os.path\n",
        "\n",
        "def cover_text(image_path, store_path, df, list_redact, color='black', width=2): \n",
        "  # img_path - the file path of the pdf or image currently reading\n",
        "  # store_path - the path where you want to store the redacted image\n",
        "  # df - the dataframe you have all the result data and a column call \"text_clean\" that cotains the list_redact words\n",
        "  # list_redact - list of information you want to cover in the image\n",
        "  image = convert_from_path(image_path, fmt=\"PNG\")[0]\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  for word in list_redact:\n",
        "    (x, y, w, h) = (df.loc[df['text_clean'] == word, 'left'], df.loc[df['text_clean'] == word, 'top'], df.loc[df['text_clean'] == word, 'width'], df.loc[df['text_clean'] == word, 'height'])\n",
        "    draw.rectangle((x, y, x + w, y + h), fill=color, width=width) \n",
        "    \n",
        "  file_name = f'{os.path.splitext(os.path.basename(image_path))[0]}_redacted' # get the original name of the image\n",
        "  image.save(f'{store_path}{file_name}.jpg', 'JPEG') # save the image into \n"
      ],
      "metadata": {
        "id": "dyydPwxaKAi5"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deal with situation that one pdf have more that 1 page\n",
        "# input page number in order to reduce that page\n",
        "\n",
        "import PIL\n",
        "from PIL import ImageDraw\n",
        "import os.path\n",
        "\n",
        "def cover_text(image_path, page_num, store_path, df, list_redact, color='green', width=2): \n",
        "  # img_path - the file path of the pdf or image currently reading\n",
        "  # page_num - the page of pdf that you want to block text\n",
        "  # store_path - the path where you want to store the redacted image\n",
        "  # df - the dataframe you have all the result data and a column call \"text_clean\" that cotains the list_redact words\n",
        "  # list_redact - list of information you want to cover in the image\n",
        "  image = convert_from_path(image_path, fmt=\"PNG\")[page_num]\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  for word in list_redact:\n",
        "    (x, y, w, h) = (df.loc[df['text_clean'] == word, 'left'], df.loc[df['text_clean'] == word, 'top'], df.loc[df['text_clean'] == word, 'width'], df.loc[df['text_clean'] == word, 'height'])\n",
        "    draw.rectangle((x, y, x + w, y + h), fill=color, width=width) \n",
        "\n",
        "  file_name = f'{os.path.splitext(os.path.basename(image_path))[0]}_redacted' # get the original name of the image\n",
        "  image.save(f'{store_path}{file_name}_page_{page_num}.jpg', 'JPEG') # save the image into "
      ],
      "metadata": {
        "id": "_hFkARkrlK1N"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deal with situation that one pdf have more that 1 page\n",
        "# input page number in order to reduce that page\n",
        "# use the pdf file name as a folder name to create a folder and store images into that folder\n",
        "\n",
        "import PIL\n",
        "from PIL import ImageDraw\n",
        "import os.path\n",
        "\n",
        "def cover_text(image_path, page_num, store_path, df, list_redact, color='green', width=2): \n",
        "  # img_path - the file path of the pdf or image currently reading\n",
        "  # page_num - the page of pdf that you want to block text\n",
        "  # store_path - the path where you want to store the redacted image\n",
        "  # df - the dataframe you have all the result data and a column call \"text_clean\" that cotains the list_redact words\n",
        "  # list_redact - list of information you want to cover in the image\n",
        "  image = convert_from_path(image_path, fmt=\"PNG\")[page_num]\n",
        "  draw = ImageDraw.Draw(image)\n",
        "  for word in list_redact:\n",
        "    (x, y, w, h) = (df.loc[df['text_clean'] == word, 'left'], df.loc[df['text_clean'] == word, 'top'], df.loc[df['text_clean'] == word, 'width'], df.loc[df['text_clean'] == word, 'height'])\n",
        "    draw.rectangle((x, y, x + w, y + h), fill=color, width=width) \n",
        "  folder_name = os.path.splitext(os.path.basename(image_path))[0]\n",
        "  store_path = os.path.join(store_path, folder_name)\n",
        "  if not os.path.exists(store_path):\n",
        "      os.mkdir(dir)  \n",
        "   # get the original name of the image\n",
        "  image.save(f'{store_path}{folder_name}_page_{page_num}.jpg', 'JPEG') # save the image into "
      ],
      "metadata": {
        "id": "2b7yZM69TLYA"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_output = \"/content/drive/MyDrive/000ABTT/RUBICONMD /Output Folder/\" # Linqing's drive path output"
      ],
      "metadata": {
        "id": "11OFC1wFQ5-_"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = img_path\n",
        "store_path = file_path_output\n",
        "df = data_df\n",
        "# list_redact\n",
        "cover_text(image_path, store_path, df, list_redact)"
      ],
      "metadata": {
        "id": "BxBsq2myRDun"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "page_num = 0\n",
        "cover_text(image_path, page_num, store_path, df, list_redact, color='green', width=2)"
      ],
      "metadata": {
        "id": "tEPvg1sZRcQf"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "boy5HXmZUdvf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}